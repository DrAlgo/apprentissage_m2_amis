{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Rappels de Probabilités"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Définition**\n",
    "On définit un **espace de probabilité** en donnant :\n",
    "1.  Un ensemble $\\Omega$, l'ensemble des *événements élémentaires*, dont les parties sont appelées les *événements*. On note $\\cal E$ l'ensemble des événements.\n",
    "2. Une *probabilité* $P$, c'est-à-dire une application $P$ qui associe à chaque événement $A$ un nombre réel  \n",
    "  * (i)   $\\forall A \\in {\\cal E}, 0 \\leq P(A) \\leq 1$  \n",
    "  * (ii)  $P(\\Omega) = 1$\n",
    "  * (iii) Si $A_1, A_2, \\dots$ dont des événements disjoints deux à deux alors\n",
    "  $$P(\\bigcup_{i=1}^\\infty A_i) = \\sum_{i=0}^{+\\infty} P(A_i)$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Un cas particulier de (ii) est le cas\n",
    "$$P(A \\cup B) = P(A) + P(B)$$\n",
    "pour deux événements $A$ et $B$ disjoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Si $A$ et $B$ ne sont pas disjoints on peut écrire\n",
    "$$P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$$\n",
    "<img src=\"images/inter.gif\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Si $A_1, A_2, \\dots, A_n$ forment une *partition* de l'ensemble des événements on peut écrire que\n",
    "$$P(A) = P(A \\cap A_1) +  P(A \\cap A_2) + \\dots + P(A \\cap A_n)$$\n",
    "<img src=\"images/partition.png\" width=300>\n",
    "(**loi des probabilités totales**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Remarque : techniquement quand $\\Omega$ est non dénombrable on ne peut pas définir $P$ sur toutes les parties \n",
    "de $\\Omega$ mais nous ferons abstraction de cette difficulté dans ce cours et \n",
    "supposerons toujours que tout est *mesurable*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exemples** Dans chaque cas on définit un ensemble $\\Omega$ et une probabilité $P$ qui satisfont les conditions ci-dessus de sorte à modéliser une expérience aléatoire.\n",
    "\n",
    "**Exemple 1.**\n",
    "On veut jeter un dé à six faces bien équilibré. Pour modéliser cette expérience aléatoire on définit $\\Omega = \\{1,2,3,4,5, 6\\}$ et on pose $P(\\{i\\}) = \\frac1{6}$ pour chaque $1 \\leq i \\leq 6$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Un événement est par exemple $A$ : \"on obtient un résultat pair\".  \n",
    "Formellement $A = \\{2,4,6\\}$ et\n",
    "$$P(A) = P(\\{2,4,6\\}) = P(\\{2\\}) + P(\\{4\\}) + P(\\{6\\}) = \\frac1{6} + \\frac1{6} + \\frac1{6} = \\frac1{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exemple 2.**\n",
    "On veut jeter deux dés à six faces bien équilibrés de couleur différentes. Pour modéliser cette expérience aléatoire on définit $\\Omega = \\{ (1,1), (1,2) , \\dots (i,j) ,\\dots, (6,6)\\}$  (36 éléments) et on pose $P(\\{(i,j)\\}) = \\frac1{36}$ pour chaque $1 \\leq i,j \\leq 6$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exemple 3**. On lance un dé jusqu'à obtenir $6$ et on veut compter le nombre $n$ de tirages nécessaires. Pour modéliser cette expérience aléatoire \n",
    "on définit  $\\Omega= \\mathbb N^*$  et on pose  $P({n}) = (\\frac5{6})^{n-1} \\cdot\\frac1{6}$  pour chaque $n \\geq 1$.\n",
    "Comme $\\sum_{n=1}^{\\infty} (\\frac5{6})^n = 6$ on a bien $P(\\Omega) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La probabilité de l'événement B: \"le nombre de tirages nécessaires jusqu'à obtenir 6 est impair\" est \n",
    "$$P(B) = P(\\{1,3,5,\\cdots\\}) = \\sum_{n=0}^{+\\infty} P(\\{2n+1\\})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$P(B) = \\sum_{n=1}^{+\\infty} (\\frac5{6})^{2n} \\cdot \\frac1{6} = \\frac1{6}  \\sum_{n=0}^{+\\infty} ((\\frac5{6})^2)^{n}$$ \n",
    "$$P(B) = \\frac1{6} \\cdot \\frac1{1 - (\\frac5{6})^2} = \\frac6{11} \\approx 0.54$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exemple 4.** On désire cette fois ci tirer au hasard un nombre réel dans $[0,+\\infty[$. \n",
    "On pose pour tout ensemble (mesurable) $A \\subset [0,+\\infty[$\n",
    "$$P(A) = \\int_A \\lambda e^{-\\lambda x} dx$$ \n",
    "où $\\lambda$ est un paramètre $>0$.\n",
    "On appelle cette probabilité la *loi exponentielle de paramètre $\\lambda$* et elle correspond à la probabilité temporelle d'advenir pour des événements sans mémoire, comme par exemple la désintégration d'un atome ou le claquage d'une ampoule.\n",
    "\n",
    "Notez que pour tout instant $t$ on a $P(\\{t\\})=0$ mais que si $0 \\leq t_1 < t_2$ alors \n",
    "$$P([t_1,t_2]) = \\int_{t_1}^{t_2}  \\lambda e^{-\\lambda x} dx > 0$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2) Variables aléatoires\n",
    "Une *variable aléatoire* est une application \n",
    "$$ X : \\Omega \\rightarrow \\mathbb R$$\n",
    "(ou parfois dans d'autres ensembles d'arrivée)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Si $B$ est une partie de $\\mathbb R$ on note \n",
    "$$P(X \\in B) \\text{ pour } P(X^{-1}(B))$$\n",
    "-- c'est la mesure de l'ensemble des événements élémentaires $\\omega$ tels que $X(\\omega ) \\in B$\n",
    "et on dit *la probabilité que $X$ appartienne à $B$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "On peut de la même façon écrire $P(X > 2)$ pour $P(X \\in ]2 ,+ \\infty[)$ par exemple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La *loi* de la variable aléatoire $X$ est la probabilité $\\mu$ définie sur $\\mathbb R$ par\n",
    "$$\\mu(A) = P( X \\in A )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**exemple** On reprend l'exemple du lancer des deux dés à 6 faces avec $\\Omega = \\{(i,j), 1 \\leq i,j \\leq 6 \\}$  \n",
    "On définit $X : \\Omega \\rightarrow \\mathbb R$ par $X(i,j) = i + j$.  \n",
    "$X$ est une variable aléatoire qui correspond à la somme des deux dés.  \n",
    "La *loi* de $X$ est la probabilité sur $\\mathbb R$ (et non sur $\\Omega$) qui a une masse non nulle sur les entiers de $2$ à $12$.\n",
    "On a par exemple \n",
    "$$\\mu_X(\\{5\\}) = P(X=5) = P(\\{(1,4)\\}) + P(\\{(2,3)\\}) + P(\\{(3,2)\\}) + P(\\{(4,1)\\}) =  \\frac4{36} = \\frac1{9}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Définition** (**indépendance**) Deux variables aléatoires $X$ et $Y$ (définies sur le même espace $\\Omega$) sont dites indépendantes si pour tous les coupes d'événements $(A,B)$ de $\\mathbb R$ on a\n",
    "$$P(\\{X \\in A\\} \\cap \\{Y \\in B\\}) = P(X \\in A) \\cdot P(X \\in B) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**exemple** Dans l'exemple des deux dés, on peut vérifier que les v.a. définies par $X_1(i,j) = i$ et $X_2(i,j) = j$ sont indépendantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "NB : **en pratique** quand on veut étudier des variables aléaoires $X$, $Y$, $Z$ étant indépendantes\n",
    "    ou autre conditions de ce type, nous avons un théorème (Kolmogorov 1932) qui nous permet de donner l'existence d'un espace probabilisé $(\\Omega, P)$ sur lequel ces variables aléatoires sont définies et satisfont les propriété requises. Il suffit de savoir que cet espace existe mais ensuite il n'intervient pas et on considère les lois sur $\\mathbb R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**exemple** Je veux considérer de façon indépendante le lancer d'un dé (variable aléatoire $D$ à valeurs dans $\\{1,2,3,4,5,6\\} \\subset \\mathbb R$ et l'instant $T$ de désintégration d'un atome (loi exponentielle de paramètre $\\lambda$, voir ci-dessus).  \n",
    "Le théorème nous assure l'existence d'un $\\Omega$ et de $P$ sur lesquel on peut définir $D$ et $T$ ayant les lois prescrites  et étant indépendantes. En pratique on écrit des probabilités comme\n",
    "$$P( D=3 \\text{ et } T > 3)$$\n",
    "dans lequel $\\Omega$ n'intervient plus de façon explicite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3) Espérance, variance, covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "L'**espérance** d'une variable aléatoire $X$ définie sur $(\\Omega,P)$ est sa moyenne:\n",
    "$$E(X) = \\int_\\Omega X dP$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Si $\\Omega$ est fini ou dénombrable, $\\Omega = \\{\\omega_1, \\omega_2, \\dots\\}$ alors l'intégrale s'écrit plus simplement\n",
    "$$E(X) = \\sum_{n=0}^\\infty X(\\omega) P(\\{\\omega\\})$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "De même si $X$ prend un nombre fini ou dénombrable de valeurs $x_1, x_2, \\cdots$ on peut écrire \n",
    "$$E(X) = \\sum_{n=0}^\\infty x_i \\cdot P(X = x_i)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**exemple** Si $X$ correspond au lancement d'un seul dé bien équilibré alors\n",
    "$$E(X) = 1\\cdot P(X=1) + 2\\cdot P(X=2) + 3\\cdot P(X=3) + 4\\cdot P(X=4) + 5\\cdot P(X=5) + 6\\cdot P(X=6) $$  \n",
    "$$ = \\frac{1+2+3+4+5+6}{6} = 3,5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Si $X$ est définie par sa loi de probabilité à densité $f$ sur $\\mathbb R$, c'est à dire\n",
    "$$P(X \\in A) =  \\int_{\\mathbb R} xd\\mu_X(x) =  \\int_A f(x)dx$$\n",
    "alors\n",
    "$$E(X) = \\int_A x \\cdot f(x)dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**exemple** Si T suit une loi exponentielle de paramètre $\\lambda$ alors\n",
    "$$E(T) =  \\int_{\\mathbb R} x d\\mu_T(x) = \\int_{-\\infty}^{+ \\infty} x \\lambda e^{-\\lambda x} dx = \\frac1{\\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Linéarité de l'espérance**\n",
    "L'espérance est linéaire, si $a$ et $b$ sont des constantes alors\n",
    "$$E(aX+bY)=aE(X)+bE(Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Propriété** (admis) Si $X$ et $Y$ sont des v.a. indépendantes alors\n",
    "$$E(X\\cdot Y)=E(X)\\cdot E(Y)$$\n",
    "Attention la réciproque est fausse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La **variance** d'une variable aléatoire $X$ est\n",
    "$$V(X) = E( (X-E(X))^2 )$$ \n",
    "C'est la moyenne du carré des écarts à la moyenne.  \n",
    "L**'écart type** de $X$ est défini par $\\sqrt{V(X)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Propriété** (preuve:exo)\n",
    "$$V(X) = E(X^2) - (E(X))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Propriété** Si $X$ et $Y$ sont indépendantes alors $V(X+Y) = V(X) + V(Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La **covariance** de deux variables aléatoires $X$ et $Y$ est la quantité\n",
    "$$Cov(X,Y) = E( (X-E(X))(Y-E(Y)))$$\n",
    "La Covariance mesure une corrélation entre $X$ et $Y$ (quand $X>E(X)$ est au dessus de sa moyenne, \n",
    "est-ce que $Y>E(Y)$, etc.)  \n",
    "Si $X$ et $Y$ sont indépendantes alors $Cov(X,Y)=0$ mais la réciproque est fausse (indépendance implique pas de corrélation mais pas l'inverse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4)  Probabilités conditionnelles\n",
    "\n",
    "Soient $A$ et $B$ deux événements.\n",
    "La probabilité conditionnelle  de $A$ sachant $B$ (sachant que $B$ s'est réalisé) est définie dans\n",
    "le cas où $P(B) >0$.\n",
    "\n",
    "$$P(A \\vert B) = \\frac{P(A \\cap B)}{P(B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**exemple** sur le lancé d'un dé : \n",
    "B : \"le résultat est différent de 1\"  et A : \"le résultat est impair\"  \n",
    "alors $$P(A \\vert B) =\\frac{P(\\{3, 5\\})}{P(\\{2,3,4,5,6\\})} = \\frac{2/6}{5/6} = \\frac{2}{5} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Attention : quand on considère une probabilité conditionnelle, on définit une nouvelle probabilité $P_B$ sur $\\Omega$, c'est-à-dire qu'on change l'expérience aléatoire.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**formule de Bayes**\n",
    "On suppose $A,B$ deux événements de probabilité strictement positive.\n",
    "On peut écrire $$P(A \\cap B) = P(A \\vert B) \\cdot P(B) =  P(B \\vert A) \\cdot P(A)$$\n",
    "On en déduit que\n",
    "$$P(A \\vert B) = \\frac{ P(B \\vert A) \\cdot P(B)  }{P(A) }$$\n",
    "qui est la formule de Bayes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**exemple** \n",
    "Considérons une expérience aléatoire où on tire une personne au hasard dans une population et on fait un test pour détecter la présence d'un virus. On suppose que le virus infecte 0,01% des gens. Le test lui, donne une mauvaise réponse 1% des fois sur les personnes ayant le virus et 0.1% des fois sur les personnes n'ayant pas le virus.\n",
    "\n",
    "$V$ : \"la personne est atteinte par le virus\" et $T$ : \"le test est positif\".\n",
    "\n",
    "Sachant que le test est positif sur la personne tirée aléatoirement, quelle est la probabilité qu'elle porte le virus ?\n",
    "\n",
    "-> exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**version de la loi des probabilités totales avec probabilités conditionnelles**\n",
    "\n",
    "Si $A_1, A_2, \\dots, A_n$ forment une *partition* de l'ensemble des événements on peut écrire que\n",
    "$$P(A) = P(A \\cap A_1) +  P(A \\cap A_2) + \\dots + P(A \\cap A_n)$$\n",
    "donc\n",
    "$$P(A) = P(A | A_1)\\cdot P(A_1 +  P(A \\cap A_2) + \\dots + P(A \\cap A_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Loi des grands nombres\n",
    "**Théorème** Soient $X_1, X_2, \\cdots X_n, \\cdots$ des variables aléatoires *indépendantes et de même loi* avec de plus $E(|X_1|) < + \\infty$. Alors on a convergence presque partout\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\frac1{n} \\sum_{i=1}^n X_i = E(X_1) $$\n",
    "En particulier on a pout tout $\\epsilon > 0$\n",
    "$$ \\lim_{n \\rightarrow \\infty} P\\left( E(X_1) - \\epsilon < \\frac1{n} \\sum_{i=1}^n X_i < E(X_1) - \\epsilon \\right) = 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
